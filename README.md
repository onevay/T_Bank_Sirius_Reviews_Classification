# T_Bank_Sirius_Reviews_Classification
Решение кейса для отбора на смену по ML

## Авторазметка
Для авторазметки воспользуемся мини-ансамблей из эвристического метода, готовой zero-shot модели, косинусного сходства между описательным словарем и отзывом и генеративной LLM модели. Первые 3 способа проверялись отдельно и показали хороший результат там, где они явно уверены. Их применение поможет сэкономить ресурсы и позволит использовать более сильную модель LLM без ущерба во времени.
Для валидации оспользовалась выборка из случайных 100 экземпляров, размеченных вручную. Разметка показала следующее распределение по категориям:

<a href="https://ibb.co/zT2ktZ8K"><img src="https://i.ibb.co/yck3xqhL/image.png" alt="image" border="0"></a>

После анализа становится понятно, что разметка получилась довольно хорошей на основных категориях, но ошибочно занесла чать отзывов в классы, которых вообще нет в отзывах, как пример "бытовую технику". Во многом это может повлиять на обучение модели, поэтому я принял решение исключить эти категории в обучении: они не помогут в качественном прогнозировании на новых данных, а только ослабят существующую модель

## Классификация

Первым делом опробовали LORA дообучение классификатора, основанного на берт, разморозив слои внимания, без доп улучшений получили f1=0.79
Дальнейшие эксперименты:

**BERT FULL TUNING** (Лучший результат, f1=0.82)

<a href="https://ibb.co/fwQ8pFY"><img src="https://i.ibb.co/95GwTg9/full-tuning-bert.jpg" alt="full-tuning-bert" border="0"></a>

**MLP с эмбеддингами из BERT**

<a href="https://ibb.co/N2dq7C9F"><img src="https://i.ibb.co/s9d4tRKg/mlp.jpg" alt="mlp" border="0"></a>

**LSTM**

<a href="https://ibb.co/wZFQKx2r"><img src="https://i.ibb.co/spd1FMXJ/lstm.jpg" alt="lstm" border="0"></a>

**BERT+LSTM**
Сделано в качестве эксперимента, модель еще можно улучшить, но даже при переобучении показала результат 0.8 на данных, которые не видела ранее

## Проблемы и способы их решения

Одной из основных проблем стал дисбаланс классов, решалась с помощью Over Sampling, весов для категорий и Focal Loss. Переобучение последней модели можно решить с помощью отключения ettintion слоя после lstm и более высоким значением dropout, но я это не успел сделать, оставив реализацию на уровне идеи. Для дальнейшего масштабирования необходимо добавить данные по абсолютно всем катогориям и воспользоваться готовым решением для них. Для валидации были размечены данные из test, они использовались, чтобы обучаться абсолютно на всех данных из train и не упустить важную информацию

`более детальная информация по отдельным фрагментам расписана в блокнотах`
