{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Fy7HlLgcHtL5XFepW-WIGi3bC_QJ_AlG",
      "authorship_tag": "ABX9TyMRzTMZlHkCMzD5O16KDvIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onevay/T_Bank_Sirius_Reviews_Classification/blob/main/BERT_LORA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT\n",
        "Уже есть размеченные данные для train для обучения. Также была создана разметка для test, но она нужна исключительно для валидации. Первым делом попробуем обучить классификатор на нескольких полносвязных слоях с извлечением признаков с помощью bert модели. Для русского языка есть несколько хороших реализаций берт, но для начала можно попробовать **DeepPavlov**"
      ],
      "metadata": {
        "id": "mb3kZCkMmSn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch transformers imbalanced-learn peft accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00jUxGu2wfpr",
        "outputId": "5b25847a-1f32-4329-fe1e-e9584ee4ea6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BERT_NAME = 'DeepPavlov/rubert-base-cased'"
      ],
      "metadata": {
        "id": "kV3LmGFswiSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "metadata": {
        "id": "1p2J17NjzMow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BERT_NAME = 'DeepPavlov/rubert-base-cased'\n",
        "categories = [\n",
        "    \"одежда\", \"нет товара\", \"украшения и аксессуары\",\n",
        "    \"товары для детей\", \"текстиль\"\n",
        "]"
      ],
      "metadata": {
        "id": "T-c8Hjc2wlRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/train_labeled.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "test_labels = pd.read_csv('/content/drive/MyDrive/test_labeled.csv')\n",
        "test_data.index.name, test_labels.index.name = 'index', 'index'\n",
        "\n",
        "#test разделен на метки и текстовые данные, нужно объединить\n",
        "train_data = train_data[train_data['predicted_category'].isin(categories)].copy()\n",
        "test_data = test_data.merge(test_labels, on='index')\n",
        "test_data = test_data[test_data['predicted_category'].isin(categories)].copy()"
      ],
      "metadata": {
        "id": "nZQ1v1TBwtD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "finkVB8a0d1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BERT_NAME)\n",
        "\n",
        "#спецсимволы не помогут классификатору, поэтому удаляем их\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^а-яёa-z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "train_data['text_clean'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['text_clean'] = test_data['text'].apply(preprocess_text)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(categories)\n",
        "train_data['label'] = label_encoder.transform(train_data['predicted_category'])\n",
        "test_data['label'] = label_encoder.transform(test_data['predicted_category'])"
      ],
      "metadata": {
        "id": "Vp5OPy4rErI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = Counter(train_data['label'])\n",
        "class_weights = torch.tensor([1.0 / class_counts[i] for i in range(len(categories))], device=device)\n",
        "class_weights = class_weights / class_weights.sum() * len(categories)"
      ],
      "metadata": {
        "id": "X0iVFnnlFxSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        #в датасете сразу будем отдавать токены и информацию об объекта (токен, маску и метку)\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ],
      "metadata": {
        "id": "ICgaX1zXF0M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как для валидации авторазметки LLM моделью мной использовалась ручная валидация выборки в 100 объектов, я заметил сильный дисбаланс классов, поэтому обучить классификатор качественно получится лишь на некоторых категориях, в которых возможно достать нужную часть данных. А для правильной выборки без перевеса в сторону `одежды` использую **RandomOverSampler**"
      ],
      "metadata": {
        "id": "TknjsfDhr0oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(\n",
        "    np.array(train_data['text_clean']).reshape(-1, 1),\n",
        "    np.array(train_data['label'])\n",
        ")\n",
        "\n",
        "train_dataset = AdvancedTextDataset(\n",
        "    X_resampled.flatten().tolist(),\n",
        "    y_resampled.tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = AdvancedTextDataset(\n",
        "    test_data['text_clean'].tolist(),\n",
        "    test_data['label'].tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "w5A6--HuKW-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#доп методом от дисбаласа будет кастомная ошибка\n",
        "class FocalLoss(torch.nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ],
      "metadata": {
        "id": "G3ivsd1PKaJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По стандарту в классификаторе ставим полносвязный слой и слой выхода, после проб определился уровень, необходимый для dropout"
      ],
      "metadata": {
        "id": "bkHGgaqv0JjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModel.from_pretrained(BERT_NAME).to(device)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.FEATURE_EXTRACTION\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config).to(device)\n",
        "\n",
        "classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(0.2),\n",
        "    torch.nn.Linear(model.config.hidden_size, 256),\n",
        "    torch.nn.LayerNorm(256),\n",
        "    torch.nn.GELU(),\n",
        "    torch.nn.Dropout(0.1),\n",
        "    torch.nn.Linear(256, len(categories))\n",
        ").to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TMrOzKkKd1a",
        "outputId": "6bbca97b-a53a-49ce-e462-84cab404d8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(input_ids, attention_mask, labels=None):\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
        "    logits = classifier(pooled_output)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "    return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "bKkAfO_FK79D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для **BERT** возьмем коэффициент для шага, чтобы не влиять сильно на смещение в пользу отдельных классов при создании признаков классификатору"
      ],
      "metadata": {
        "id": "5_rF38Wu00u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": model.parameters(), \"lr\": 5e-5},\n",
        "        {\"params\": classifier.parameters(), \"lr\": 1e-4}\n",
        "    ],\n",
        "    weight_decay=0.001\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=[5e-5, 1e-4],\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=10,\n",
        "    pct_start=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "0Ett_YYZK-OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#можно увеличить количество эпох, но дальше улучшения будут гораздо медленнее\n",
        "num_epochs = 10\n",
        "best_f1 = 0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        loss, logits = forward_pass(input_ids, attention_mask, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = forward_pass(input_ids, attention_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "    accuracy = accuracy_score(test_labels, test_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        torch.save(model.state_dict(), 'best_model_lora.pth')\n",
        "        torch.save(classifier.state_dict(), 'best_classifier.pth')\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuaZV6y9OBvq",
        "outputId": "4859ea2a-3f98-4cff-e12f-b0c951b794ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 758/758 [01:52<00:00,  6.73it/s, loss=0.637]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.5747, F1: 0.1736, Accuracy: 0.2119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 758/758 [01:53<00:00,  6.70it/s, loss=0.00883]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 0.0481, F1: 0.6206, Accuracy: 0.5841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 758/758 [01:51<00:00,  6.77it/s, loss=0.677]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Loss: 0.0226, F1: 0.6496, Accuracy: 0.6210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 758/758 [01:51<00:00,  6.77it/s, loss=0.00285]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Loss: 0.0157, F1: 0.7060, Accuracy: 0.6837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 758/758 [01:52<00:00,  6.74it/s, loss=0.00374]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Loss: 0.0096, F1: 0.7261, Accuracy: 0.7062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 758/758 [01:52<00:00,  6.76it/s, loss=0.000507]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Loss: 0.0070, F1: 0.7830, Accuracy: 0.7730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 758/758 [01:52<00:00,  6.76it/s, loss=0.000273]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Loss: 0.0065, F1: 0.7848, Accuracy: 0.7783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 758/758 [01:51<00:00,  6.78it/s, loss=0.00226]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Loss: 0.0044, F1: 0.7891, Accuracy: 0.7825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 758/758 [01:52<00:00,  6.74it/s, loss=0.00025]\n",
            "Validation: 100%|██████████| 895/895 [00:59<00:00, 15.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Loss: 0.0035, F1: 0.7893, Accuracy: 0.7839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 758/758 [01:52<00:00,  6.75it/s, loss=0.0565]\n",
            "Validation: 100%|██████████| 895/895 [01:01<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Loss: 0.0035, F1: 0.7905, Accuracy: 0.7842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Сохранение результатов"
      ],
      "metadata": {
        "id": "QTeFSVnH1V-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model_lora.pth'))\n",
        "classifier.load_state_dict(torch.load('best_classifier.pth'))\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "\n",
        "final_preds = []\n",
        "final_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        logits = forward_pass(input_ids, attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        final_preds.extend(preds.cpu().numpy())\n",
        "        final_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_data['predicted'] = label_encoder.inverse_transform(final_preds)\n",
        "test_data.reset_index()[['index', 'predicted']].to_csv('test_predictions_lora.csv', index=False)\n",
        "\n",
        "torch.save(model.state_dict(), 'final_model_lora_weights.pth')\n",
        "torch.save(classifier.state_dict(), 'final_classifier_weights.pth')\n",
        "\n",
        "print(\"Test predictions saved to test_predictions_lora.csv\")\n",
        "print(\"Model weights saved to final_model_lora_weights.pth and final_classifier_weights.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFJQuXllU9-K",
        "outputId": "75766f23-03fc-4165-dd1b-afc2969473bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Final Evaluation: 100%|██████████| 895/895 [01:01<00:00, 14.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test predictions saved to test_predictions_lora.csv\n",
            "Model weights saved to final_model_lora_weights.pth and final_classifier_weights.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По результатам видно, что модель иногда путает детские товары и одижду, отсутствие товара и одежду. Это очевидно, ведь этих классов больше всего, а в товарах для детей пояляется и одежда, как, например, в одежде описывается текстиль. По итогу на **LORA** дообучении с 2-мя слоями nn удалось достичь `f1 = 0.79`, что неплохо, учитывая, что мы сильно не прорабатывали модель, а лишь поработали с дисбалансом классов"
      ],
      "metadata": {
        "id": "qnQ4RDvcQT9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Test Results:\")\n",
        "print(classification_report(final_labels, final_preds, target_names=categories))\n",
        "print(f\"Weighted F1 Score: {f1_score(final_labels, final_preds, average='weighted'):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(final_labels, final_preds):.4f}\")\n",
        "\n",
        "conf_matrix = pd.crosstab(\n",
        "    label_encoder.inverse_transform(final_labels),\n",
        "    label_encoder.inverse_transform(final_preds),\n",
        "    rownames=['Actual'],\n",
        "    colnames=['Predicted']\n",
        ")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "N-nUdOPEWeGo",
        "outputId": "ed37cbb9-0010-48e2-e4ce-c610230a0132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Results:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "                одежда       0.72      0.85      0.78      1730\n",
            "            нет товара       0.89      0.80      0.85      4794\n",
            "украшения и аксессуары       0.39      0.48      0.43       526\n",
            "      товары для детей       0.00      0.00      0.00        11\n",
            "              текстиль       0.30      0.47      0.37        97\n",
            "\n",
            "              accuracy                           0.78      7158\n",
            "             macro avg       0.46      0.52      0.48      7158\n",
            "          weighted avg       0.80      0.78      0.79      7158\n",
            "\n",
            "Weighted F1 Score: 0.7905\n",
            "Accuracy: 0.7842\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted               нет товара  одежда  текстиль  товары для детей  \\\n",
            "Actual                                                                   \n",
            "нет товара                    1465     221        33                 2   \n",
            "одежда                         515    3851       337                 5   \n",
            "текстиль                        62     202       251                 2   \n",
            "товары для детей                 0       4         6                 0   \n",
            "украшения и аксессуары           1      40        10                 0   \n",
            "\n",
            "Predicted               украшения и аксессуары  \n",
            "Actual                                          \n",
            "нет товара                                   9  \n",
            "одежда                                      86  \n",
            "текстиль                                     9  \n",
            "товары для детей                             1  \n",
            "украшения и аксессуары                      46  \n"
          ]
        }
      ]
    }
  ]
}